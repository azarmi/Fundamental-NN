{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation Class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageData_loader(dataset_path, train_name, test_name, one_hot_encode=False):\n",
    "    train_inputs = []\n",
    "    train_targets = []\n",
    "    test_inputs = []\n",
    "    test_targets = []\n",
    "\n",
    "    train_class_path = dataset_path + '\\\\' + train_name\n",
    "    test_class_path = dataset_path + '\\\\' + test_name\n",
    "    train_items = []\n",
    "    test_items = []\n",
    "    num_class =0\n",
    "    for cls in os.listdir(train_class_path):\n",
    "        num_class += 1\n",
    "        # x_train\n",
    "        counter = 0\n",
    "        current_class = train_class_path + '\\\\' + cls\n",
    "        for file in os.listdir(current_class):\n",
    "            im = img.imread(current_class + '\\\\' + file)\n",
    "            im = np.reshape(im,(im.shape[0] * im.shape[1]))\n",
    "            train_inputs.append(im)\n",
    "            counter += 1\n",
    "        train_items.append(counter)\n",
    "        # x_test\n",
    "        counter = 0\n",
    "        current_class = test_class_path + '\\\\' + cls\n",
    "        for file in os.listdir(current_class):\n",
    "            im = img.imread(current_class + '\\\\' + file)\n",
    "            im = np.reshape(im,(im.shape[0] * im.shape[1]))\n",
    "            test_inputs.append(im)\n",
    "            counter += 1\n",
    "        test_items.append(counter)\n",
    "\n",
    "    for cls in range(num_class):\n",
    "        if one_hot_encode:\n",
    "            one_hot = np.ndarray([num_class], dtype='int')\n",
    "            one_hot.fill(0)\n",
    "            one_hot[cls] = 1\n",
    "            # y_train\n",
    "            for i in range(train_items[cls]):\n",
    "                train_targets.append(one_hot)\n",
    "            # y_test\n",
    "            for i in range(test_items[cls]):\n",
    "                test_targets.append(one_hot)\n",
    "        else:\n",
    "            # y_train\n",
    "            for i in range(train_items[cls]):\n",
    "                train_targets.append(cls)\n",
    "            # y_test\n",
    "            for i in range(test_items[cls]):\n",
    "                test_targets.append(cls)\n",
    "            \n",
    "    return np.array(train_inputs), np.array(train_targets), np.array(test_inputs), np.array(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageData_normalize(image):\n",
    "#     image /= 255\n",
    "    image *= 2\n",
    "    image -= 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Spliter():\n",
    "    def __init__(self, dataset, test_split=0, batch_size=0, validation_split=.0, shuffle=False):\n",
    "        self.num_samples = dataset[0].shape[0]\n",
    "        dsIndx = np.arange(self.num_samples)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(dsIndx)\n",
    "        self.dataset = dataset[0][dsIndx], dataset[1][dsIndx]\n",
    "        self.num_valid = int(self.num_samples * validation_split)\n",
    "        self.num_test  = int(self.num_samples * test_split)\n",
    "        self.num_train = self.num_samples - self.num_test - self.num_valid\n",
    "        if self.num_valid:\n",
    "            self.valid_data = self.dataset[0][self.num_train:self.num_train+self.num_valid], self.dataset[1][self.num_train:self.num_train+self.num_valid]\n",
    "            \n",
    "        if self.num_test:\n",
    "            self.test_data = self.dataset[0][self.num_train+self.num_valid::], self.dataset[1][self.num_train+self.num_valid::]\n",
    "            \n",
    "        self.train_data = self.dataset[0][0:self.num_train], self.dataset[1][0:self.num_train]\n",
    "        \n",
    "        if batch_size == 0:\n",
    "            self.batch_size = self.num_samples\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def train_batchs(self):\n",
    "        for i in range(0, self.num_train, self.batch_size):\n",
    "            yield (self.train_data[0][i:i+self.batch_size], self.train_data[1][i:i+self.batch_size]) \n",
    "    \n",
    "    def info(self):\n",
    "        print('Number of Samples: {}\\n| # Train: {} | # Validation: {} | # Test: {}'.format\n",
    "             (self.num_samples, self.num_train, self.num_valid, self.num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers):\n",
    "        w1 = np.random.randn(layers[0], layers[1]) / np.sqrt(layers[0])\n",
    "        w2 = np.random.randn(layers[1], layers[2]) / np.sqrt(layers[1])\n",
    "        w3 = np.random.randn(layers[2], layers[3]) / np.sqrt(layers[2])\n",
    "        b1 = np.zeros((1, layers[1]))\n",
    "        b2 = np.zeros((1, layers[2]))\n",
    "        b3 = np.zeros((1, layers[3]))\n",
    "        self.weights= {}\n",
    "        self.set_weights(w1,b1,w2,b2,w3,b3)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        w1=self.weights['w1']\n",
    "        b1=self.weights['b1']\n",
    "        w2=self.weights['w2']\n",
    "        b2=self.weights['b2']\n",
    "        w3=self.weights['w3']\n",
    "        b3=self.weights['b3']\n",
    "        return w1, b1, w2, b2, w3, b3\n",
    "    \n",
    "    def set_weights(self, w1,b1,w2,b2,w3,b3):\n",
    "        self.weights['w1'] = w1\n",
    "        self.weights['b1'] = b1\n",
    "        self.weights['w2'] = w2\n",
    "        self.weights['b2'] = b2\n",
    "        self.weights['w3'] = w3\n",
    "        self.weights['b3'] = b3\n",
    "\n",
    "    #     @loss.setter\n",
    "    def loss(self, loss_type):\n",
    "        self.loss_type = loss_type\n",
    "    def loss_fun(self, y_hat, y):\n",
    "        num_sample = y.shape[0]\n",
    "        if self.loss_type == 'categorical-crossentropy':\n",
    "            return -np.sum(y * np.log(y_hat)) #/num_sample\n",
    "        elif self.loss_type == 'sparse-categorical-crossentropy':\n",
    "            return -np.sum(np.log(y_hat[range(y.shape[0]),y])) # /num_sample\n",
    "    def d_loss_fun(self, y_hat, y):\n",
    "        num_sample = y.shape[0]\n",
    "        if self.loss_type == 'categorical-crossentropy':\n",
    "            return y_hat - y #/num_sample\n",
    "        elif self.loss_type == 'sparse-categorical-crossentropy':\n",
    "            d_loss = y_hat.copy()\n",
    "            d_loss[range(y.shape[0]), y] -=1\n",
    "            return d_loss      \n",
    "    def accuracy_fun(self, y_hat, y):        \n",
    "        if self.loss_type == 'categorical-crossentropy':\n",
    "            return (np.argmax(y , axis=1) == np.argmax(y_hat, axis=1))\n",
    "        elif self.loss_type == 'sparse-categorical-crossentropy':\n",
    "            return  (y == np.argmax(y_hat, axis=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w1, b1, w2, b2, w3, b3 = self.get_weights()\n",
    "        hidden1 = np.maximum(0, np.dot(x, w1) + b1)\n",
    "        hidden2 = np.maximum(0, np.dot(hidden1, w2) + b2)\n",
    "        linear = np.dot(hidden2, w3) + b3\n",
    "        normalize_exp = np.exp(linear - np.max(linear))\n",
    "        y_hat = normalize_exp / np.sum(normalize_exp, axis=1, keepdims=True)\n",
    "        return y_hat \n",
    "    \n",
    "    def test(self, x, y):\n",
    "        num_test = x.shape[0]\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fun(y_hat, y) / num_test\n",
    "        accuracy = self.accuracy_fun(y_hat, y).mean()\n",
    "        print('Test Loss: {:.4f}, Accuracy: {:.1f}%'.format(loss, accuracy*100))\n",
    "    \n",
    "    def backward(self, Data, epochs, lr, reg_rate=0., verbose=1, trend_plot=False):\n",
    "        w1, b1, w2, b2, w3, b3 = self.get_weights()\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        valid_loss = []\n",
    "        valid_accuracy = []\n",
    "        for epoch in range(epochs):   \n",
    "            iter_loss = 0.\n",
    "            correct = 0\n",
    "            iterations = 0    \n",
    "            for (x, y) in Data.train_batchs():\n",
    "                iterations +=1\n",
    "                batch_size = x.shape[0]\n",
    "                \n",
    "                hidden1 = np.maximum(0, np.dot(x, w1) + b1)\n",
    "                hidden2 = np.maximum(0, np.dot(hidden1, w2) + b2)\n",
    "                linear = np.dot(hidden2, w3) + b3\n",
    "                normalize_exp = np.exp(linear - np.max(linear))\n",
    "                y_hat = normalize_exp / np.sum(normalize_exp, axis=1, keepdims=True)\n",
    "\n",
    "                reg_loss = .5*reg_rate * np.sum(w1*w1) + .5*reg_rate * np.sum(w2*w2) + .5*reg_rate * np.sum(w3*w3)\n",
    "                delta_loss = self.loss_fun(y_hat, y) + reg_loss\n",
    "                iter_loss += delta_loss / batch_size \n",
    "\n",
    "                d_loss = self.d_loss_fun(y_hat, y)\n",
    "\n",
    "                d_w3 = np.dot(hidden2.T, d_loss)\n",
    "                d_b3 = np.sum(d_loss, axis=0, keepdims=True)\n",
    "\n",
    "                d_hidden2 = np.dot(d_loss, w3.T)\n",
    "                d_hidden2[hidden2 <=0 ] = 0\n",
    "\n",
    "                d_w2 = np.dot(hidden1.T, d_hidden2)\n",
    "                d_b2 = np.sum(d_hidden2, axis=0, keepdims=True)\n",
    "\n",
    "                d_hidden1 = np.dot(d_hidden2, w2.T)\n",
    "                d_hidden1[hidden1 <=0 ] = 0\n",
    "\n",
    "                d_w1 = np.dot(x.T, d_hidden1)\n",
    "                d_b1 = np.sum(d_hidden1, axis=0, keepdims=True)\n",
    "\n",
    "                w1 += -lr * (d_w1 + 2*reg_rate * w1)\n",
    "                b1 += -lr * d_b1\n",
    "                w2 += -lr * (d_w2 + 2*reg_rate * w2)\n",
    "                b2 += -lr * d_b2\n",
    "                w3 += -lr * (d_w3 + 2*reg_rate * w3)\n",
    "                b3 += -lr * d_b3\n",
    "\n",
    "                correct += self.accuracy_fun(y_hat, y).sum()\n",
    "    \n",
    "            train_loss.append(iter_loss / iterations)\n",
    "            train_accuracy.append(correct / Data.num_train)\n",
    "\n",
    "            self.set_weights(w1,b1,w2,b2,w3,b3)\n",
    "\n",
    "            # validation phase\n",
    "            valid_inf = ''\n",
    "            if Data.num_valid:\n",
    "                x, y = Data.valid_data\n",
    "                y_hat = self.forward(x)\n",
    "                loss = self.loss_fun(y_hat, y) / Data.num_valid\n",
    "                accuracy = self.accuracy_fun(y_hat, y).mean()\n",
    "                valid_loss.append(loss)\n",
    "                valid_accuracy.append(accuracy)\n",
    "            \n",
    "                valid_inf = ', Validation Loss: {:.4f}, Validation Accuracy :{:.1f}%'.format(valid_loss[-1], 100*valid_accuracy[-1])\n",
    "\n",
    "            if epoch % verbose == 0:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}, Accurecy: {:.1f}% {}'.format\n",
    "                     (epoch, epochs, train_loss[-1], 100*train_accuracy[-1], valid_inf))\n",
    "\n",
    "        print('Training Done.')\n",
    "\n",
    "        if trend_plot:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.plot(train_loss, label = 'Trainig Loss')\n",
    "            plt.plot(valid_loss, label = 'Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.plot(train_accuracy, label = 'Trainig Accuracy')\n",
    "            plt.plot(valid_accuracy, label = 'Validation Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'C:\\Users\\Golenaz\\Jupyter\\Dataset\\mnist_png'\n",
    "train_name = 'training'\n",
    "test_name = 'testing'\n",
    "\n",
    "X_train, y_train, X_test, y_test = ImageData_loader(dataset_path, train_name, test_name, one_hot_encode=True)\n",
    "\n",
    "X_train = ImageData_normalize(X_train)\n",
    "X_test = ImageData_normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 60000\n",
      "| # Train: 60000 | # Validation: 0 | # Test: 0\n"
     ]
    }
   ],
   "source": [
    "prepared_data = Data_Spliter((X_train, y_train), test_split=0, batch_size=32, validation_split=0, shuffle=True)   \n",
    "prepared_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Loss: 0.4098, Accurecy: 87.5% \n",
      "Epoch [1/10], Loss: 0.1911, Accurecy: 94.3% \n",
      "Epoch [2/10], Loss: 0.1402, Accurecy: 95.8% \n",
      "Epoch [3/10], Loss: 0.1127, Accurecy: 96.7% \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f21decb2484d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'categorical-crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrend_plot\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-83d7337071f1>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, Data, epochs, lr, reg_rate, verbose, trend_plot)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0md_b1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_hidden1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mw1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_w1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreg_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                 \u001b[0mb1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_b1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mw2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_w2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreg_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "nn = NN([784, 128, 32, 10])\n",
    "nn.loss('categorical-crossentropy')\n",
    "nn.backward(prepared_data, epochs=10, lr= .001, reg_rate=0.001, verbose=1, trend_plot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2892, Accuracy: 11.4%\n"
     ]
    }
   ],
   "source": [
    "nn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "-1.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 8, 2, 8, 7], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\Golenaz\\Desktop\\Handwrite\"\n",
    "imgs = []\n",
    "for f in os.listdir(folder_path):\n",
    "    file_path = folder_path + '\\\\' + f\n",
    "    im = img.imread(file_path)\n",
    "    temp = np.asarray(im)\n",
    "    temp = temp[:,:,0]\n",
    "    temp[temp<.38] = 0\n",
    "    temp[temp>.38] = 1\n",
    "    temp = 1-temp\n",
    "    temp *= 2\n",
    "    temp -= 1\n",
    "    temp = temp.reshape(28* 28)\n",
    "    imgs.append(temp)\n",
    "    \n",
    "image = np.stack(imgs)\n",
    "print(image.shape)\n",
    "\n",
    "i = 9\n",
    "print(image[i].min(), image[i].max())\n",
    "plt.imshow(image[i].reshape(28, 28), cmap='gray')\n",
    "y_hat = nn.forward(image)\n",
    "np.argmax(y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-590d3d90d154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
